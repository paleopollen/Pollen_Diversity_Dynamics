{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train CNNs and obtain classification accuracies"
      ],
      "metadata": {
        "id": "0fS5tss1h5kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image classifier (H-CNN)"
      ],
      "metadata": {
        "id": "b_mzUEU4icgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "\n",
        "data_dir = 'E:Grass_Images_Split_01'\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=10,\n",
        "                                             shuffle=True, num_workers=0)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "-yu21c-0mrkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Models"
      ],
      "metadata": {
        "id": "6ma6HRySnzLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, models, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import math\n",
        "\n",
        "SEED = 123\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# Ensure deterministic behavior\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Data transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation((-90, 90)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'train_strong': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation((-90, 90)),\n",
        "        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
        "        transforms.RandomAutocontrast(p=0.5),\n",
        "        transforms.RandomEqualize(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "labeled_data_dir = \"E:Grass_Images_Split_01/\"\n",
        "unlabeled_data_dir = \"E:Grass_Images_Fossils_And_Subfossils/\"\n",
        "\n",
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, file_paths, transform=None, strong_transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transform\n",
        "        self.strong_transform = strong_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        weak_image = self.transform(image) if self.transform else image\n",
        "        strong_image = self.strong_transform(image) if self.strong_transform else image\n",
        "        return weak_image, strong_image\n",
        "\n",
        "labeled_dataset = datasets.ImageFolder(os.path.join(labeled_data_dir, 'train'), data_transforms['train'])\n",
        "labeled_dataset_size = len(labeled_dataset)\n",
        "\n",
        "unlabeled_file_paths = [os.path.join(unlabeled_data_dir, img) for img in os.listdir(unlabeled_data_dir)]\n",
        "unlabeled_dataset = UnlabeledDataset(unlabeled_file_paths, data_transforms['train'], data_transforms['train_strong'])\n",
        "unlabeled_dataset_size = len(unlabeled_dataset)\n",
        "\n",
        "mu = math.ceil(unlabeled_dataset_size / labeled_dataset_size)\n",
        "labeled_batch_size = 10\n",
        "unlabeled_batch_size = int(labeled_batch_size * mu)\n",
        "\n",
        "labeled_loader = DataLoader(labeled_dataset, batch_size=labeled_batch_size, shuffle=True, num_workers=0)\n",
        "val_dataset = datasets.ImageFolder(os.path.join(labeled_data_dir, 'val'), data_transforms['val'])\n",
        "val_loader = DataLoader(val_dataset, batch_size=labeled_batch_size, shuffle=False, num_workers=0)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=unlabeled_batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "model = models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(labeled_dataset.classes))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "lambda_u = 1\n",
        "T = 1\n",
        "threshold = 0.95\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0009, momentum=0.9)\n",
        "step_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
        "\n",
        "def sharpen(probabilities, temperature):\n",
        "    res = torch.pow(probabilities, 1.0 / temperature)\n",
        "    return torch.div(res, torch.sum(res, dim=1, keepdim=True))\n",
        "\n",
        "num_epochs = 30\n",
        "best_acc = 0.0\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        if phase == 'train':\n",
        "            iter_unlabeled = iter(unlabeled_loader)\n",
        "            for inputs_l, labels in labeled_loader:\n",
        "                inputs_l, labels = inputs_l.to(device), labels.to(device)\n",
        "                try:\n",
        "                    inputs_u_w, inputs_u_s = next(iter_unlabeled)\n",
        "                except StopIteration:\n",
        "                    iter_unlabeled = iter(unlabeled_loader)\n",
        "                    inputs_u_w, inputs_u_s = next(iter_unlabeled)\n",
        "                inputs_u_w, inputs_u_s = inputs_u_w.to(device), inputs_u_s.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs_l = model(inputs_l)\n",
        "                    loss_l = criterion(outputs_l, labels).mean()\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        outputs_u_w = model(inputs_u_w)\n",
        "                        pseudo_labels = torch.softmax(outputs_u_w, dim=1)\n",
        "                        pseudo_labels = sharpen(pseudo_labels, T)\n",
        "                        max_probs, pseudo_labels = torch.max(pseudo_labels, dim=1)\n",
        "                        mask = max_probs.ge(threshold).float()\n",
        "                        pseudo_labels = pseudo_labels[mask > 0]\n",
        "                        inputs_u_s = inputs_u_s[mask > 0]\n",
        "\n",
        "                    if len(inputs_u_s) > 0:\n",
        "                        outputs_u_s = model(inputs_u_s)\n",
        "                        loss_u = criterion(outputs_u_s, pseudo_labels)\n",
        "                        loss_u = (lambda_u * loss_u).mean()\n",
        "                    else:\n",
        "                        loss_u = 0\n",
        "\n",
        "                    loss = loss_l + loss_u\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs_l.size(0)\n",
        "                _, preds = torch.max(outputs_l, 1)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            step_lr_scheduler.step()\n",
        "\n",
        "        else:\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels).mean()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if phase == 'train':\n",
        "            epoch_loss = running_loss / labeled_dataset_size\n",
        "            epoch_acc = running_corrects.double() / labeled_dataset_size\n",
        "        elif phase == 'val':\n",
        "            epoch_loss = running_loss / len(val_dataset)\n",
        "            epoch_acc = running_corrects.double() / len(val_dataset)\n",
        "\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        if phase == 'val' and epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "print('Best val Acc: {:4f}'.format(best_acc))\n",
        "model.load_state_dict(best_model_wts)\n",
        "torch.save(model.state_dict(), 'Semi_Supervised_Grass_Images_Split01.pth')"
      ],
      "metadata": {
        "id": "ZBAwXRgIh37Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, models, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import copy\n",
        "import random\n",
        "import math\n",
        "\n",
        "SEED = 123\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Data transforms\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation((-90, 90)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'train_strong': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation((-90, 90)),\n",
        "        transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
        "        transforms.RandomAutocontrast(p=0.5),\n",
        "        transforms.RandomEqualize(p=0.5),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "labeled_data_dir = \"E:Grass_Patches_Split_01/\"\n",
        "unlabeled_data_dir = \"E:Grass_Patches_Fossils_And_Subfossils/\"\n",
        "\n",
        "def collect_image_paths(directory):\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "    return image_paths\n",
        "\n",
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, file_paths, transform=None, strong_transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.transform = transform\n",
        "        self.strong_transform = strong_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_paths[idx]\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except (PermissionError, FileNotFoundError) as e:\n",
        "            print(f\"Error opening image {img_path}: {e}\")\n",
        "            return None, None\n",
        "\n",
        "        weak_image = self.transform(image) if self.transform else image\n",
        "        strong_image = self.strong_transform(image) if self.strong_transform else image\n",
        "        return weak_image, strong_image\n",
        "\n",
        "labeled_dataset = datasets.ImageFolder(os.path.join(labeled_data_dir, 'train'), data_transforms['train'])\n",
        "labeled_dataset_size = len(labeled_dataset)\n",
        "\n",
        "unlabeled_file_paths = collect_image_paths(unlabeled_data_dir)\n",
        "unlabeled_dataset = UnlabeledDataset(unlabeled_file_paths, data_transforms['train'], data_transforms['train_strong'])\n",
        "unlabeled_dataset_size = len(unlabeled_dataset)\n",
        "\n",
        "mu = math.ceil(unlabeled_dataset_size / labeled_dataset_size)\n",
        "labeled_batch_size = 10\n",
        "unlabeled_batch_size = int(labeled_batch_size * mu)\n",
        "\n",
        "labeled_loader = DataLoader(labeled_dataset, batch_size=labeled_batch_size, shuffle=True, num_workers=0)\n",
        "val_dataset = datasets.ImageFolder(os.path.join(labeled_data_dir, 'val'), data_transforms['val'])\n",
        "val_loader = DataLoader(val_dataset, batch_size=labeled_batch_size, shuffle=False, num_workers=0)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=unlabeled_batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "model = models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(labeled_dataset.classes))\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "lambda_u = 1\n",
        "T = 1\n",
        "threshold = 0.95\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0009, momentum=0.9)\n",
        "step_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
        "\n",
        "def sharpen(probabilities, temperature):\n",
        "    res = torch.pow(probabilities, 1.0 / temperature)\n",
        "    return torch.div(res, torch.sum(res, dim=1, keepdim=True))\n",
        "\n",
        "num_epochs = 30\n",
        "best_acc = 0.0\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train':\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "\n",
        "        if phase == 'train':\n",
        "            iter_unlabeled = iter(unlabeled_loader)\n",
        "            for inputs_l, labels in labeled_loader:\n",
        "                inputs_l, labels = inputs_l.to(device), labels.to(device)\n",
        "                try:\n",
        "                    inputs_u_w, inputs_u_s = next(iter_unlabeled)\n",
        "                    if inputs_u_w is None or inputs_u_s is None:\n",
        "                        continue\n",
        "                except StopIteration:\n",
        "                    iter_unlabeled = iter(unlabeled_loader)\n",
        "                    inputs_u_w, inputs_u_s = next(iter_unlabeled)\n",
        "                    if inputs_u_w is None or inputs_u_s is None:\n",
        "                        continue\n",
        "                inputs_u_w, inputs_u_s = inputs_u_w.to(device), inputs_u_s.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs_l = model(inputs_l)\n",
        "                    loss_l = criterion(outputs_l, labels).mean()\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        outputs_u_w = model(inputs_u_w)\n",
        "                        pseudo_labels = torch.softmax(outputs_u_w, dim=1)\n",
        "                        pseudo_labels = sharpen(pseudo_labels, T)\n",
        "                        max_probs, pseudo_labels = torch.max(pseudo_labels, dim=1)\n",
        "                        mask = max_probs.ge(threshold).float()\n",
        "                        pseudo_labels = pseudo_labels[mask > 0]\n",
        "                        inputs_u_s = inputs_u_s[mask > 0]\n",
        "\n",
        "                    if len(inputs_u_s) > 0:\n",
        "                        outputs_u_s = model(inputs_u_s)\n",
        "                        loss_u = criterion(outputs_u_s, pseudo_labels)\n",
        "                        loss_u = (lambda_u * loss_u).mean()\n",
        "                    else:\n",
        "                        loss_u = 0\n",
        "\n",
        "                    loss = loss_l + loss_u\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs_l.size(0)\n",
        "                _, preds = torch.max(outputs_l, 1)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            step_lr_scheduler.step()\n",
        "\n",
        "        else:\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels).mean()\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if phase == 'train':\n",
        "            epoch_loss = running_loss / labeled_dataset_size\n",
        "            epoch_acc = running_corrects.double() / labeled_dataset_size\n",
        "        elif phase == 'val':\n",
        "            epoch_loss = running_loss / len(val_dataset)\n",
        "            epoch_acc = running_corrects.double() / len(val_dataset)\n",
        "\n",
        "        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "        if phase == 'val' and epoch_acc > best_acc:\n",
        "            best_acc = epoch_acc\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "print('Best val Acc: {:4f}'.format(best_acc))\n",
        "model.load_state_dict(best_model_wts)\n",
        "torch.save(model.state_dict(), 'Semi_Supervised_Grass_Patches_Split01.pth')"
      ],
      "metadata": {
        "id": "NME_ZIfznjdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Classification Accuracies (H-CNN)"
      ],
      "metadata": {
        "id": "E40Nyox9nsl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "#Â Forward Pass Images to the network to get (1) Softmax probabilities and (2) best classification prediction\n",
        "\n",
        "PATH = \"Semi_Supervised_Grass_Images_Split01.pth\"\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names));\n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    #probs = torch.softmax(logits, 1).detach().cpu().numpy()\n",
        "    probs = logits.detach().cpu().numpy()\n",
        "    probs = torch.tensor(probs)\n",
        "    probs = probs.mean(0)\n",
        "    probs = torch.softmax(probs, 0).detach().cpu().numpy()\n",
        "    # Take average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "# Validation dir\n",
        "\n",
        "val_dir = 'E:Grass_Images_Split_01/val'\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)\n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "\n",
        "        results.append((image_dir, class_id))\n",
        "        print(f'Predict {image_dir} as class {class_id}')\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)\n",
        "print(all_preds.shape, all_preds.dtype, labels.shape, labels.dtype)\n",
        "print('Accuracy: {}'.format((all_preds.argmax(1) == labels).mean()))"
      ],
      "metadata": {
        "id": "4Ef52MxhnYva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Confusion Matrix (H-CNN)\n",
        "\n"
      ],
      "metadata": {
        "id": "bRH_LGQaoA2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the main folder\n",
        "main_folder_path = \"E:Grass_Images_Split_01/\"\n",
        "\n",
        "# Get a list of all subfolders in the main folder\n",
        "subfolders = [f.path for f in os.scandir(main_folder_path) if f.is_dir()]\n",
        "\n",
        "# Dictionary to store the number of folders in each subfolder\n",
        "subfolder_counts = {}\n",
        "\n",
        "# Iterate through each subfolder and count the number of subfolders they contain\n",
        "for subfolder in subfolders:\n",
        "    subfolder_name = os.path.basename(subfolder)\n",
        "    inner_folders = [f.path for f in os.scandir(subfolder) if f.is_dir()]\n",
        "    subfolder_counts[subfolder_name] = len(inner_folders)\n",
        "\n",
        "# Print the results\n",
        "for subfolder_name, count in subfolder_counts.items():\n",
        "    print(f\"Subfolder: {subfolder_name}, Number of inner folders: {count}\")"
      ],
      "metadata": {
        "id": "2xyIL3DSsmYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "updated_taxon_names = [\"Elymus ciliaris\",\"Eragrostis mexicana\",\"Agrostis quinqueseta\",\"Agrostis trachyphylla\",\"Agrostis volkensii\",\"Amphibromus neesii\",\"Andropogon amethystinus\",\"Andropogon chrysostachyus\",\"Heteropogon contortus\",\"Andropogon lima\",\"Andropogon schirensis\",\"Anthoxanthum nivale\",\"Aristida megapotamica\",\"Bothriochloa bladhii\",\"Urochloa brizantha\",\"Brachypodium flexum\",\"Bromus auleticus\",\"Bromus orcuttianus\",\"Bromus ciliatus\",\"Bromus lanatus\",\"Bromus leptoclados\",\"Bromus hordeaceus subsp. thominei\",\"Calamagrostis epigejos\",\"Chrysopogon fallax\",\"Cymbopogon nardus\",\"Cynodon dactylon\",\"Dactylis glomerata\",\"Digitaria abyssinica\",\"Echinopogon caespitosus\",\"Ehrharta erecta\",\"Ehrharta erecta var. natalensis\",\"Eleusine coracana\",\"Eleusine jaegeri\",\"Exotheca abyssinica\",\"Pseudobromus africanus\",\"Festuca costata\",\"Ampelodesmos mauritanicus\",\"Isachne mauritiana\",\"Koeleria capensis\",\"Lolium perenne\",\"Melica onoei\",\"Miscanthidium violaceum\",\"Oplismenus compositus\",\"Oplismenus hirtellus\",\"Cenchrus clandestinus\",\"Pentameris borussica\",\"Phalaris arundinacea\",\"Poa anceps\",\"Poa leptoclada\",\"Poa schimperiana\",\"Tripidium arundinaceum\",\"Secale cereale\",\"Setaria megaphylla\",\"Oldeania alpina\",\"Sorghum halepense\",\"Sporobolus michauxianus\",\"Austrostipa flavescens\",\"Koordersiochloa longiarista\",\"Themeda triandra\",\"Zea mays\"];\n",
        "\n",
        "plt.rcParams['font.family'] = 'Arial'\n",
        "\n",
        "TaxonName = updated_taxon_names\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(labels, all_preds.argmax(1))\n",
        "\n",
        "# Sort TaxonName alphabetically and get sorted indices\n",
        "sorted_indices = np.argsort(TaxonName)\n",
        "sorted_TaxonName = np.array(TaxonName)[sorted_indices]\n",
        "\n",
        "# Reorder the confusion matrix rows and columns based on sorted indices\n",
        "cm_sorted = cm[sorted_indices, :][:, sorted_indices]\n",
        "\n",
        "# Normalize confusion matrix by the sum of rows\n",
        "cm_normalized = cm_sorted.astype('float') / cm_sorted.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Ensure no division by zero\n",
        "cm_normalized[np.isnan(cm_normalized)] = 0\n",
        "\n",
        "# Compute confusion matrix for calculating accuracies\n",
        "cm_full = confusion_matrix(labels, all_preds.argmax(1))\n",
        "\n",
        "# Calculate the accuracy for each taxon\n",
        "accuracies = np.diag(cm_full) / cm_full.sum(axis=1) * 100\n",
        "\n",
        "# Reorder accuracies to match the sorted taxon names\n",
        "reordered_accuracies = accuracies[sorted_indices]\n",
        "\n",
        "# Function to italicize taxon names except \"var.\" and \"subsp.\"\n",
        "def italicize_name(name):\n",
        "    words = name.split()\n",
        "    italicized_words = [f\"$\\\\it{{{word}}}$\" if word not in [\"var.\", \"subsp.\"] else word for word in words]\n",
        "    return \" \".join(italicized_words)\n",
        "\n",
        "# Get the specimen counts sorted according to sorted_TaxonName\n",
        "sorted_specimen_counts = [subfolder_counts[name] for name in sorted_TaxonName]\n",
        "\n",
        "# Update taxon names with accuracy in parentheses for x-axis\n",
        "updated_taxon_names_with_accuracy = [\n",
        "    f\"{italicize_name(name)} ({reordered_accuracies[idx]:.0f})\"\n",
        "    for idx, name in enumerate(sorted_TaxonName)\n",
        "]\n",
        "\n",
        "# Update taxon names with specimen count in parentheses for y-axis\n",
        "updated_taxon_names_with_counts = [\n",
        "    f\"{italicize_name(name)} ({sorted_specimen_counts[idx]})\"\n",
        "    for idx, name in enumerate(sorted_TaxonName)\n",
        "]\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(35, 27), dpi=300)\n",
        "ax = sns.heatmap(cm_normalized, annot=False, fmt=\".2f\", vmax=1, xticklabels=updated_taxon_names_with_accuracy, yticklabels=updated_taxon_names_with_counts, cmap='cividis', linewidths=.0)\n",
        "ax.set_xlabel('Predicted Taxon (%)', fontsize=32)\n",
        "ax.set_ylabel('True Taxon (N)', fontsize=32)\n",
        "\n",
        "# Adjust tick parameters\n",
        "plt.tick_params(axis='x', which='major', labelsize=30)\n",
        "plt.tick_params(axis='y', which='major', labelsize=30)\n",
        "\n",
        "# Modify colorbar font size\n",
        "cbar = ax.collections[0].colorbar\n",
        "cbar.ax.tick_params(labelsize=30)\n",
        "\n",
        "# Italicize the tick labels\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right', rotation_mode='anchor', fontsize=24)\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=24)\n",
        "\n",
        "ax.xaxis.set_tick_params(pad=10)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UjzK1XPhnaQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_images_01 = cm_normalized"
      ],
      "metadata": {
        "id": "aU1ENZbLnaWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate Image Directories and Softmax Probabilities, and get sorted probability vectors\n",
        "\n",
        "import numpy as np\n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "\n",
        "Image_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Image_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "# Image (Whole Images) Labels\n",
        "labels.sort()\n",
        "ImageLabels = labels"
      ],
      "metadata": {
        "id": "cNxWVUrihPrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Classification Accuracies (P-CNN)"
      ],
      "metadata": {
        "id": "onWRDj52oR7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the main folder\n",
        "main_folder_path = \"E:Grass_Patches_Split_01/\"\n",
        "\n",
        "# Get a list of all subfolders in the main folder\n",
        "subfolders = [f.path for f in os.scandir(main_folder_path) if f.is_dir()]\n",
        "\n",
        "# Dictionary to store the number of folders in each subfolder\n",
        "subfolder_counts = {}\n",
        "\n",
        "# Iterate through each subfolder and count the number of subfolders they contain\n",
        "for subfolder in subfolders:\n",
        "    subfolder_name = os.path.basename(subfolder)\n",
        "    inner_folders = [f.path for f in os.scandir(subfolder) if f.is_dir()]\n",
        "    subfolder_counts[subfolder_name] = len(inner_folders)\n",
        "\n",
        "# Print the results\n",
        "for subfolder_name, count in subfolder_counts.items():\n",
        "    print(f\"Subfolder: {subfolder_name}, Number of inner folders: {count}\")"
      ],
      "metadata": {
        "id": "sDo8Nn5Ts5WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "\n",
        "PATH = \"Semi_Supervised_Grass_Patches_Split01.pth\"\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, len(class_names));\n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);\n",
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def predict(model, image_dir):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    logits = model(images)\n",
        "    #probs = torch.softmax(logits, 1).detach().cpu().numpy()\n",
        "    probs = logits.detach().cpu().numpy()\n",
        "    probs = torch.tensor(probs)\n",
        "    probs = probs.mean(0)\n",
        "    probs = torch.softmax(probs, 0).detach().cpu().numpy()\n",
        "    # Take average and get final prediction\n",
        "    # avg_probs = probs.mean(0)\n",
        "    avg_probs = probs\n",
        "    final_pred = avg_probs.argmax()\n",
        "    return final_pred, avg_probs\n",
        "\n",
        "# Validation dir\n",
        "\n",
        "val_dir = 'E:Grass_Patches_Split_01/val'\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir, '*'))\n",
        "model.eval()\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "results = []\n",
        "all_preds = []\n",
        "labels = []\n",
        "all_image_dirs = []\n",
        "\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        label = class_map[class_name]\n",
        "        labels.append(label)\n",
        "        all_image_dirs.append(image_dir)\n",
        "        class_id, probs = predict(model, image_dir)\n",
        "        all_preds.append(probs)\n",
        "\n",
        "        results.append((image_dir, class_id))\n",
        "        print(f'Predict {image_dir} as class {class_id}')\n",
        "\n",
        "all_preds = np.stack(all_preds, 0)\n",
        "labels = np.array(labels)\n",
        "print(all_preds.shape, all_preds.dtype, labels.shape, labels.dtype)\n",
        "print('Accuracy: {}'.format((all_preds.argmax(1) == labels).mean()))"
      ],
      "metadata": {
        "id": "7XIJ9NOLoQgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Confusion Matrix (P-CNN)"
      ],
      "metadata": {
        "id": "c-razmvKogP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "updated_taxon_names = [\"Elymus ciliaris\",\"Eragrostis mexicana\",\"Agrostis quinqueseta\",\"Agrostis trachyphylla\",\"Agrostis volkensii\",\"Amphibromus neesii\",\"Andropogon amethystinus\",\"Andropogon chrysostachyus\",\"Heteropogon contortus\",\"Andropogon lima\",\"Andropogon schirensis\",\"Anthoxanthum nivale\",\"Aristida megapotamica\",\"Bothriochloa bladhii\",\"Urochloa brizantha\",\"Brachypodium flexum\",\"Bromus auleticus\",\"Bromus orcuttianus\",\"Bromus ciliatus\",\"Bromus lanatus\",\"Bromus leptoclados\",\"Bromus hordeaceus subsp. thominei\",\"Calamagrostis epigejos\",\"Chrysopogon fallax\",\"Cymbopogon nardus\",\"Cynodon dactylon\",\"Dactylis glomerata\",\"Digitaria abyssinica\",\"Echinopogon caespitosus\",\"Ehrharta erecta\",\"Ehrharta erecta var. natalensis\",\"Eleusine coracana\",\"Eleusine jaegeri\",\"Exotheca abyssinica\",\"Pseudobromus africanus\",\"Festuca costata\",\"Ampelodesmos mauritanicus\",\"Isachne mauritiana\",\"Koeleria capensis\",\"Lolium perenne\",\"Melica onoei\",\"Miscanthidium violaceum\",\"Oplismenus compositus\",\"Oplismenus hirtellus\",\"Cenchrus clandestinus\",\"Pentameris borussica\",\"Phalaris arundinacea\",\"Poa anceps\",\"Poa leptoclada\",\"Poa schimperiana\",\"Tripidium arundinaceum\",\"Secale cereale\",\"Setaria megaphylla\",\"Oldeania alpina\",\"Sorghum halepense\",\"Sporobolus michauxianus\",\"Austrostipa flavescens\",\"Koordersiochloa longiarista\",\"Themeda triandra\",\"Zea mays\"];\n",
        "\n",
        "plt.rcParams['font.family'] = 'Arial'\n",
        "\n",
        "TaxonName = updated_taxon_names\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(labels, all_preds.argmax(1))\n",
        "\n",
        "# Sort TaxonName alphabetically and get sorted indices\n",
        "sorted_indices = np.argsort(TaxonName)\n",
        "sorted_TaxonName = np.array(TaxonName)[sorted_indices]\n",
        "\n",
        "# Reorder the confusion matrix rows and columns based on sorted indices\n",
        "cm_sorted = cm[sorted_indices, :][:, sorted_indices]\n",
        "\n",
        "# Normalize confusion matrix by the sum of rows\n",
        "cm_normalized = cm_sorted.astype('float') / cm_sorted.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Ensure no division by zero\n",
        "cm_normalized[np.isnan(cm_normalized)] = 0\n",
        "\n",
        "# Compute confusion matrix for calculating accuracies\n",
        "cm_full = confusion_matrix(labels, all_preds.argmax(1))\n",
        "\n",
        "# Calculate the accuracy for each taxon\n",
        "accuracies = np.diag(cm_full) / cm_full.sum(axis=1) * 100\n",
        "\n",
        "# Reorder accuracies to match the sorted taxon names\n",
        "reordered_accuracies = accuracies[sorted_indices]\n",
        "\n",
        "# Function to italicize taxon names except \"var.\" and \"subsp.\"\n",
        "def italicize_name(name):\n",
        "    words = name.split()\n",
        "    italicized_words = [f\"$\\\\it{{{word}}}$\" if word not in [\"var.\", \"subsp.\"] else word for word in words]\n",
        "    return \" \".join(italicized_words)\n",
        "\n",
        "# Get the specimen counts sorted according to sorted_TaxonName\n",
        "sorted_specimen_counts = [subfolder_counts[name] for name in sorted_TaxonName]\n",
        "\n",
        "# Update taxon names with accuracy in parentheses for x-axis\n",
        "updated_taxon_names_with_accuracy = [\n",
        "    f\"{italicize_name(name)} ({reordered_accuracies[idx]:.0f})\"\n",
        "    for idx, name in enumerate(sorted_TaxonName)\n",
        "]\n",
        "\n",
        "# Update taxon names with specimen count in parentheses for y-axis\n",
        "updated_taxon_names_with_counts = [\n",
        "    f\"{italicize_name(name)} ({sorted_specimen_counts[idx]})\"\n",
        "    for idx, name in enumerate(sorted_TaxonName)\n",
        "]\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(35, 27), dpi=300)\n",
        "ax = sns.heatmap(cm_normalized, annot=False, fmt=\".2f\", vmax=1, xticklabels=updated_taxon_names_with_accuracy, yticklabels=updated_taxon_names_with_counts, cmap='cividis', linewidths=.0)\n",
        "ax.set_xlabel('Predicted Taxon (%)', fontsize=32)\n",
        "ax.set_ylabel('True Taxon (N)', fontsize=32)\n",
        "\n",
        "# Adjust tick parameters\n",
        "plt.tick_params(axis='x', which='major', labelsize=30)\n",
        "plt.tick_params(axis='y', which='major', labelsize=30)\n",
        "\n",
        "# Modify colorbar font size\n",
        "cbar = ax.collections[0].colorbar\n",
        "cbar.ax.tick_params(labelsize=30)\n",
        "\n",
        "# Italicize the tick labels\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right', rotation_mode='anchor', fontsize=24)\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=24)\n",
        "\n",
        "ax.xaxis.set_tick_params(pad=10)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hXRF1Kguocgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm_patches_01 = cm_normalized"
      ],
      "metadata": {
        "id": "bRXKW2tDoltn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate Image Directories and Softmax Probabilities, and get Sorted probability vectors\n",
        "\n",
        "import numpy as np\n",
        "Dirs_and_Scores = {}\n",
        "for i in range (len(all_image_dirs)):\n",
        "  array_dir = all_image_dirs[i]\n",
        "  array_pred = all_preds[i]\n",
        "  Dirs_and_Scores[array_dir] = array_pred\n",
        "  #Dirs_and_Scores.append(Dir_and_Score)\n",
        "\n",
        "Patch_Probs_Sorted = []\n",
        "for key in sorted(Dirs_and_Scores.keys()) :\n",
        "   #print(key , \" :: \" , Dirs_and_Scores[key])\n",
        "   #Dirs_and_Scores[key]\n",
        "   Patch_Probs_Sorted.append(Dirs_and_Scores[key])\n",
        "\n",
        "# Patch Labels\n",
        "labels.sort()\n",
        "PatchLabels = labels"
      ],
      "metadata": {
        "id": "uRbdE7MmomWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate fused classification accuracy (combining H-CNN and P-CNN)"
      ],
      "metadata": {
        "id": "4hsq3Pw7ozBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as f\n",
        "\n",
        "# Fuse softmax score vectors across the two modalities (image and patch CNNs):\n",
        "\n",
        "Fused_Probabilities = torch.tensor(Image_Probs_Sorted) * torch.tensor(Patch_Probs_Sorted)\n",
        "\n",
        "# Normalize to [0-1] probability vectors\n",
        "\n",
        "#Fused_Probabilities_Normalized = f.normalize(torch.tensor(Fused_Probabilities), p=2, dim=1)\n",
        "Fused_Probabilities_Normalized = Fused_Probabilities / Fused_Probabilities.sum(dim=1, keepdim=True)\n",
        "\n",
        "\n",
        "# Save Fused Probabilities as File\n",
        "#np.savetxt(absolute_path + \"/Fused_Probabilities_Normalized.csv\", Fused_Probabilities_Normalized, delimiter=\",\")\n",
        "\n",
        "#print(Fused_Probabilities_Normalized.shape)\n",
        "Fused_Probabilities_Normalized_array = np.array(Fused_Probabilities_Normalized)\n",
        "\n",
        "# Compute Fused Classification Accuracy\n",
        "Indices = []\n",
        "for i in range(len(Fused_Probabilities_Normalized_array)):\n",
        "  index, = np.where(Fused_Probabilities_Normalized_array[i] == Fused_Probabilities_Normalized_array[i].max())\n",
        "  Indices.append(index.item())\n",
        "\n",
        "Accuracy = []\n",
        "for i in range(len(Indices)):\n",
        "   Per_Instance_Accuracy = Indices[i] == ImageLabels[i]\n",
        "   Accuracy.append(Per_Instance_Accuracy)\n",
        "\n",
        "Fused_Classification_Accuracy = sum(Accuracy)/len(Fused_Probabilities_Normalized_array)\n",
        "print('Fused Classification Accuracy: {}'.format(Fused_Classification_Accuracy))"
      ],
      "metadata": {
        "id": "xFLPxyMYop4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "updated_taxon_names = [\"Elymus ciliaris\",\"Eragrostis mexicana\",\"Agrostis quinqueseta\",\"Agrostis trachyphylla\",\"Agrostis volkensii\",\"Amphibromus neesii\",\"Andropogon amethystinus\",\"Andropogon chrysostachyus\",\"Heteropogon contortus\",\"Andropogon lima\",\"Andropogon schirensis\",\"Anthoxanthum nivale\",\"Aristida megapotamica\",\"Bothriochloa bladhii\",\"Urochloa brizantha\",\"Brachypodium flexum\",\"Bromus auleticus\",\"Bromus orcuttianus\",\"Bromus ciliatus\",\"Bromus lanatus\",\"Bromus leptoclados\",\"Bromus hordeaceus subsp. thominei\",\"Calamagrostis epigejos\",\"Chrysopogon fallax\",\"Cymbopogon nardus\",\"Cynodon dactylon\",\"Dactylis glomerata\",\"Digitaria abyssinica\",\"Echinopogon caespitosus\",\"Ehrharta erecta\",\"Ehrharta erecta var. natalensis\",\"Eleusine coracana\",\"Eleusine jaegeri\",\"Exotheca abyssinica\",\"Pseudobromus africanus\",\"Festuca costata\",\"Ampelodesmos mauritanicus\",\"Isachne mauritiana\",\"Koeleria capensis\",\"Lolium perenne\",\"Melica onoei\",\"Miscanthidium violaceum\",\"Oplismenus compositus\",\"Oplismenus hirtellus\",\"Cenchrus clandestinus\",\"Pentameris borussica\",\"Phalaris arundinacea\",\"Poa anceps\",\"Poa leptoclada\",\"Poa schimperiana\",\"Tripidium arundinaceum\",\"Secale cereale\",\"Setaria megaphylla\",\"Oldeania alpina\",\"Sorghum halepense\",\"Sporobolus michauxianus\",\"Austrostipa flavescens\",\"Koordersiochloa longiarista\",\"Themeda triandra\",\"Zea mays\"];\n",
        "\n",
        "plt.rcParams['font.family'] = 'Arial'\n",
        "\n",
        "TaxonName = updated_taxon_names\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(labels, Indices)\n",
        "\n",
        "# Sort TaxonName alphabetically and get sorted indices\n",
        "sorted_indices = np.argsort(TaxonName)\n",
        "sorted_TaxonName = np.array(TaxonName)[sorted_indices]\n",
        "\n",
        "# Reorder the confusion matrix rows and columns based on sorted indices\n",
        "cm_sorted = cm[sorted_indices, :][:, sorted_indices]\n",
        "\n",
        "# Normalize confusion matrix by the sum of rows\n",
        "cm_normalized = cm_sorted.astype('float') / cm_sorted.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Ensure no division by zero\n",
        "cm_normalized[np.isnan(cm_normalized)] = 0\n",
        "\n",
        "# Compute confusion matrix for calculating accuracies\n",
        "cm_full = confusion_matrix(labels, all_preds.argmax(1))\n",
        "\n",
        "# Calculate the accuracy for each taxon\n",
        "accuracies = np.diag(cm_full) / cm_full.sum(axis=1) * 100\n",
        "\n",
        "# Reorder accuracies to match the sorted taxon names\n",
        "reordered_accuracies = accuracies[sorted_indices]\n",
        "\n",
        "# Function to italicize taxon names except \"var.\" and \"subsp.\"\n",
        "def italicize_name(name):\n",
        "    words = name.split()\n",
        "    italicized_words = [f\"$\\\\it{{{word}}}$\" if word not in [\"var.\", \"subsp.\"] else word for word in words]\n",
        "    return \" \".join(italicized_words)\n",
        "\n",
        "# Get the specimen counts sorted according to sorted_TaxonName\n",
        "sorted_specimen_counts = [subfolder_counts[name] for name in sorted_TaxonName]\n",
        "\n",
        "# Update taxon names with accuracy in parentheses for x-axis\n",
        "updated_taxon_names_with_accuracy = [\n",
        "    f\"{italicize_name(name)} ({reordered_accuracies[idx]:.0f})\"\n",
        "    for idx, name in enumerate(sorted_TaxonName)\n",
        "]\n",
        "\n",
        "# Update taxon names with specimen count in parentheses for y-axis\n",
        "updated_taxon_names_with_counts = [\n",
        "    f\"{italicize_name(name)} ({sorted_specimen_counts[idx]})\"\n",
        "    for idx, name in enumerate(sorted_TaxonName)\n",
        "]\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(35, 27), dpi=300)\n",
        "ax = sns.heatmap(cm_normalized, annot=False, fmt=\".2f\", vmax=1, xticklabels=updated_taxon_names_with_accuracy, yticklabels=updated_taxon_names_with_counts, cmap='cividis', linewidths=.0)\n",
        "ax.set_xlabel('Predicted Taxon (%)', fontsize=32)\n",
        "ax.set_ylabel('True Taxon (N)', fontsize=32)\n",
        "\n",
        "# Adjust tick parameters\n",
        "plt.tick_params(axis='x', which='major', labelsize=30)\n",
        "plt.tick_params(axis='y', which='major', labelsize=30)\n",
        "\n",
        "# Modify colorbar font size\n",
        "cbar = ax.collections[0].colorbar\n",
        "cbar.ax.tick_params(labelsize=30)\n",
        "\n",
        "# Italicize the tick labels\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha='right', rotation_mode='anchor', fontsize=24)\n",
        "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=24)\n",
        "\n",
        "ax.xaxis.set_tick_params(pad=10)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7NQx9UMYoxQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gBy-_Ofrip24"
      }
    }
  ]
}