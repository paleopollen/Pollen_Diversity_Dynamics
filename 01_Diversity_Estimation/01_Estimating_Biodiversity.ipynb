{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load CNNs and extract classification features"
      ],
      "metadata": {
        "id": "Jkc5U9CtD6p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# H-CNN"
      ],
      "metadata": {
        "id": "bUZ8srlDFgoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = 'Semi_Supervised_Grass_Images_Split01.pth'\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 60);\n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "coECCuQYD96l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import ConvertImageDtype\n",
        "from torchvision.transforms.functional import convert_image_dtype\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def extract_features(model, image_dir, k=100):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    feats = model(images)\n",
        "    feats = feats.detach().cpu().numpy()\n",
        "    patcho = image_dir\n",
        "\n",
        "    # Take average and get final prediction\n",
        "    # feats = feats.mean(0)\n",
        "\n",
        "    ###### Add line below to group all patch features / image\n",
        "    feats = np.sort(feats, 0)[::-1][:k].mean(0)\n",
        "    print(feats.shape)\n",
        "    return feats\n",
        "\n",
        "\n",
        "# Extract features from fossil specimens\n",
        "\n",
        "val_dir =  \"E:Grass_Images_Fossils\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir,'*'))\n",
        "\n",
        "# Modify output\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "all_image_dirs = []\n",
        "features = []\n",
        "labels = []\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        #label = class_map[class_name]\n",
        "        #labels.append(label)\n",
        "        all_image_dirs.append(image_dir)\n",
        "        feats = extract_features(model, image_dir)\n",
        "        features.append(feats)\n",
        "\n",
        "features = np.stack(features, 0)"
      ],
      "metadata": {
        "id": "WaK2J_LcEq26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_features = features"
      ],
      "metadata": {
        "id": "yP8ciWafEq5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# P-CNN"
      ],
      "metadata": {
        "id": "9V8D8mTIFjap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = 'Semi_Supervised_Grass_Patches_Split01.pth'\n",
        "\n",
        "model =  models.resnext101_32x8d(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 60);\n",
        "model.load_state_dict(torch.load(PATH));\n",
        "#model.load_state_dict(torch.load(PATH, map_location=\"cpu\"))\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "bVyq1YZDEq90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import ConvertImageDtype\n",
        "from torchvision.transforms.functional import convert_image_dtype\n",
        "import glob\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    torchvision.transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def extract_features(model, image_dir, k=100):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    patch_paths = glob.glob(os.path.join(image_dir, '*'))\n",
        "\n",
        "    images = []\n",
        "    for path in patch_paths:\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "    images = torch.stack(images, 0).to(device)\n",
        "    feats = model(images)\n",
        "    feats = feats.detach().cpu().numpy()\n",
        "    patcho = image_dir\n",
        "\n",
        "    # Take average and get final prediction\n",
        "    # feats = feats.mean(0)\n",
        "\n",
        "    ###### Add line below to group all patch features / image\n",
        "    feats = np.sort(feats, 0)[::-1][:k].mean(0)\n",
        "    print(feats.shape)\n",
        "    return feats\n",
        "\n",
        "\n",
        "# Extract features from fossil specimens\n",
        "\n",
        "val_dir =  \"E:Grass_Patches_Fossils\"\n",
        "\n",
        "val_class_dirs = glob.glob(os.path.join(val_dir,'*'))\n",
        "\n",
        "# Modify output\n",
        "model.fc = nn.Identity()\n",
        "model.eval()\n",
        "\n",
        "class_map = {name: idx for idx, name in enumerate(class_names)}\n",
        "all_image_dirs = []\n",
        "features = []\n",
        "labels = []\n",
        "for d in val_class_dirs:\n",
        "    image_dirs = glob.glob(os.path.join(d, '*'))\n",
        "    for image_dir in image_dirs:\n",
        "        class_name = os.path.basename(image_dir).split('.')[0]\n",
        "        #label = class_map[class_name]\n",
        "        #labels.append(label)\n",
        "        all_image_dirs.append(image_dir)\n",
        "        feats = extract_features(model, image_dir)\n",
        "        features.append(feats)\n",
        "\n",
        "features = np.stack(features, 0)"
      ],
      "metadata": {
        "id": "dbyQV3a6FxDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_features = features"
      ],
      "metadata": {
        "id": "wys1K6noHbCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate image and patch features and save tensor\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "fossil_image_features_tensor = torch.tensor(fossil_image_features)\n",
        "fossil_patch_features_tensor = torch.tensor(fossil_patch_features)\n",
        "\n",
        "# Concatenate the tensors\n",
        "combined_tensor = torch.cat((fossil_image_features_tensor, fossil_patch_features_tensor), dim=1)\n",
        "\n",
        "pd.DataFrame(np.array(combined_tensor)).to_csv(\"Grass_Images_Patches_Fossils_Concatenated_Features.csv\")"
      ],
      "metadata": {
        "id": "D1JbadGgHj_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Shannon entropy values for each and every pollen assemblage across Lake Rutundu sediment core"
      ],
      "metadata": {
        "id": "513FPgMHU-Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KernelDensity\n",
        "import torch\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "# Y defines different ages (along Lake Rutundu sediment core)\n",
        "Y = [20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,39,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,71,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,124,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,134,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,163,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,173,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,195,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,236,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,263,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,286,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,298,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,531,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,321,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,335,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,375,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,397,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,423,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,432,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,453,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,470,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,483,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516,516]\n",
        "\n",
        "# Depth to age mapping\n",
        "depth_to_age = {20:252.922, 39:1006.79, 71:2446.333, 124:4497.479, 134:5001.556, 163:6496.673, 173:7022.491, 195:8195.662, 236:9281.874, 256:10554.34, 263:11004.19, 286:12497.5, 298:13285.34, 321:14230.91, 335:14995.26, 375:17204.36, 397:18434.3, 423:19520.83, 432:20040.09, 453:21257.91, 470:22249.94, 483:23012.15, 516:24014.17, 531:25000.81}\n",
        "\n",
        "# Round ages in the dictionary to the nearest 5\n",
        "for depth in depth_to_age:\n",
        "    depth_to_age[depth] = 5 * round(depth_to_age[depth] / 5)\n",
        "\n",
        "# Replace depth values in Y with their corresponding ages\n",
        "ages = [depth_to_age[depth] for depth in Y]\n",
        "\n",
        "# Convert the list of ages to a tensor\n",
        "age_tensor = torch.tensor(ages)\n",
        "\n",
        "Y_tens = age_tensor\n",
        "\n",
        "# Load the datasets\n",
        "df_images = pd.read_csv(\"Grass_Images_Patches_Fossils_Concatenated_Features.csv\", header=None)\n",
        "\n",
        "# Convert DataFrames to Numpy arrays and standardize\n",
        "X_Rutundu_images = df_images.to_numpy()\n",
        "scaler = StandardScaler()\n",
        "X_Rutundu_images_std = scaler.fit_transform(X_Rutundu_images)\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=1)\n",
        "X_Rutundu_images_pca = pca.fit_transform(X_Rutundu_images_std)\n",
        "\n",
        "# Define Shannon Entropy\n",
        "def shannon_entropy(probabilities):\n",
        "    return -np.sum(probabilities * np.log(probabilities))  # Small constant to avoid log(0)\n",
        "\n",
        "# Calculate entropy for each age using the first PC\n",
        "age_entropies = {}\n",
        "for age in set(age_tensor.numpy()):\n",
        "    # Select PCA feature vectors for the current age\n",
        "    age_vectors_pca = X_Rutundu_images_pca[age_tensor == age]\n",
        "\n",
        "    # Estimate probability distribution using Kernel Density Estimation\n",
        "    kde = KernelDensity(kernel='gaussian', bandwidth=0.5).fit(age_vectors_pca)\n",
        "    log_dens = kde.score_samples(age_vectors_pca)\n",
        "    probabilities = np.exp(log_dens)\n",
        "    probabilities /= probabilities.sum()  # Normalize to sum to 1\n",
        "\n",
        "    # Calculate Shannon Entropy for the current age\n",
        "    entropy = shannon_entropy(probabilities)\n",
        "    age_entropies[age] = entropy\n",
        "\n",
        "# Sort ages and get corresponding entropies\n",
        "sorted_ages = sorted(age_entropies.keys(), reverse=False)\n",
        "entropies = [age_entropies[age] for age in sorted_ages]\n",
        "\n",
        "# Plot Entropy over Age\n",
        "plt.figure(figsize=(2, 5))\n",
        "plt.plot(entropies, sorted_ages)\n",
        "plt.ylabel('Age (cal. yr BP)')\n",
        "plt.xlabel('Shannon Entropy')\n",
        "plt.title('Shannon Entropy Over Time')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l_H0gS-DDFll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting the raw entropy measurements into Shannon diversity index values"
      ],
      "metadata": {
        "id": "CEzuQMayUreK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "\n",
        "pca = PCA(n_components=1)\n",
        "X_pca = pca.fit_transform(X_Rutundu_images_std)\n",
        "\n",
        "# Shannon Entropy\n",
        "def shannon_entropy(counts):\n",
        "    \"\"\"Calculate Shannon Entropy.\"\"\"\n",
        "    total = np.sum(counts)\n",
        "    proportions = counts / total\n",
        "    entropy = -np.sum(proportions * np.log(proportions))\n",
        "    return entropy\n",
        "\n",
        "# Calculate diversities for a given K (where K is the number of morphotypes in the assemblage)\n",
        "def calculate_diversities(K):\n",
        "    kmeans = KMeans(n_clusters=K, n_init=15, random_state=0)\n",
        "    clusters = kmeans.fit_predict(X_pca)\n",
        "    age_diversities = {}\n",
        "    for cluster in range(K):\n",
        "        ages_in_cluster = age_tensor[clusters == cluster]\n",
        "        unique_ages, counts = ages_in_cluster.unique(return_counts=True)\n",
        "        for age, count in zip(unique_ages, counts):\n",
        "            age_value = age.item()\n",
        "            if age_value not in age_diversities:\n",
        "                age_diversities[age_value] = []\n",
        "            age_diversities[age_value].append(count.item())\n",
        "    sorted_ages = sorted(age_diversities.keys())\n",
        "    diversities = [shannon_entropy(np.array(age_diversities[age])) for age in sorted_ages]\n",
        "    return diversities\n",
        "\n",
        "# Iterate over K values and calculate correlation and p-value\n",
        "K_values = range(2, 50)\n",
        "correlation_results = []\n",
        "for K in K_values:\n",
        "    diversities = calculate_diversities(K)\n",
        "    # Align entropies with diversities based on ages\n",
        "    correlation, p_value = pearsonr(diversities, entropies)\n",
        "    correlation_results.append((correlation, p_value))\n",
        "\n",
        "# Find the K value (number of morphotypes) with the maximum correlation coefficient\n",
        "optimal_K = K_values[np.argmax([result[0] for result in correlation_results])]\n",
        "optimal_correlation, optimal_p_value = correlation_results[np.argmax([result[0] for result in correlation_results])]\n",
        "\n",
        "print(\"Optimal K:\", optimal_K)\n",
        "print(\"Optimal Correlation:\", optimal_correlation)\n",
        "print(\"P-value:\", optimal_p_value)"
      ],
      "metadata": {
        "id": "vW-z0qctB-r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize morphological variability in fossil vs modern data (PC1 and PC2)"
      ],
      "metadata": {
        "id": "QhvQaCeDUDAs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "# Load fossil and modern data\n",
        "df_rutundu = pd.read_csv(\"E:Grass_Images_Patches_Fossils_Concatenated_Features.csv\", header=None)\n",
        "df_modern = pd.read_csv(\"E:Grass_Images_Patches_Concatenated_Features.csv\", header=None)\n",
        "\n",
        "# Assign labels for modern data\n",
        "modern_labels = ['Modern'] * df_modern.shape[0]\n",
        "\n",
        "# Concatenate datasets and labels\n",
        "X_combined = np.concatenate([df_rutundu, df_modern], axis=0)\n",
        "Y_combined = np.concatenate([ages, modern_labels], axis=0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_combined_scaled = scaler.fit_transform(X_combined)\n",
        "\n",
        "\n",
        "# PCA on the entire concatenated dataset (including modern data)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca_combined = pca.fit_transform(X_combined_scaled)\n",
        "\n",
        "# Sort unique labels with 'Modern' first, followed by the fossil data from youngest to oldest\n",
        "unique_labels = sorted(np.unique(Y_combined_str), key=lambda x: (x != 'Modern', float(x) if x != 'Modern' else float('inf')))\n",
        "n_labels = len(unique_labels)\n",
        "\n",
        "# Subplot grid\n",
        "n_cols = 6\n",
        "n_rows = n_labels // n_cols + (n_labels % n_cols > 0)\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(25, 18))\n",
        "fig.subplots_adjust(hspace=0.5, wspace=0.5)\n",
        "\n",
        "x_limits = [np.min(X_pca_combined[:, 0]), np.max(X_pca_combined[:, 0])]\n",
        "y_limits = [np.min(X_pca_combined[:, 1]), np.max(X_pca_combined[:, 1])]\n",
        "\n",
        "# Colors for the convex hulls and points\n",
        "hull_colors = plt.cm.viridis(np.linspace(1, 0, n_labels))\n",
        "\n",
        "for i, label in enumerate(unique_labels):\n",
        "    ax = axes[i // n_cols, i % n_cols]\n",
        "\n",
        "    # Project PCA scores for given label (depth or modern)\n",
        "    specimens_at_label = X_combined_scaled[Y_combined_str == label]\n",
        "    specimens_pca = pca.transform(specimens_at_label)\n",
        "\n",
        "    # Plot PCA results for the depth/modern\n",
        "    ax.scatter(specimens_pca[:, 0], specimens_pca[:, 1], s=3, color=hull_colors[i], alpha=0.6, label=\"\")\n",
        "\n",
        "    # Convex hull\n",
        "    if len(specimens_pca) > 2:\n",
        "        hull = ConvexHull(specimens_pca)\n",
        "        hull_points = specimens_pca[hull.vertices]\n",
        "        ax.fill(hull_points[:, 0], hull_points[:, 1], color=hull_colors[i], alpha=0.2)\n",
        "        # For closed polygon\n",
        "        for simplex in hull.simplices:\n",
        "            ax.plot(specimens_pca[simplex, 0], specimens_pca[simplex, 1], color=hull_colors[i], linewidth=1)\n",
        "        ax.plot(np.append(hull_points[:, 0], hull_points[0, 0]), np.append(hull_points[:, 1], hull_points[0, 1]), color=hull_colors[i], linewidth=1)\n",
        "\n",
        "    ax.set_title(f'{label} cal. yr BP' if label != 'Modern' else 'Modern', fontsize=14)\n",
        "    ax.set_xlabel('PC1', fontsize=12)\n",
        "    ax.set_ylabel('PC2', fontsize=12)\n",
        "\n",
        "    ax.tick_params(axis='both', which='major', labelsize=12)\n",
        "\n",
        "    ax.set_xlim(x_limits)\n",
        "    ax.set_ylim(y_limits)\n",
        "\n",
        "for j in range(i + 1, n_rows * n_cols):\n",
        "    fig.delaxes(axes[j // n_cols, j % n_cols])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n9226Z1ETsNi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}